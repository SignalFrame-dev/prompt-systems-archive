# Hallucination Boundaries in Prompt Design

**Author:** SignalFrame  
**Type:** Design ethics note  
**Status:** Living document

---

## Premise

Large language models are not neutral. They are prediction engines trained on latent bias, implicit context, and surface-level structure. In high-stakes domains, hallucination is not noiseâ€”itâ€™s *risk.*

---

## Boundaries That Protect

### âœ… Acceptable Techniques
- â€œOnly include information present in the inputâ€
- Explicit field labeling
- Output format constraints (e.g., bullet-first, label-colon)
- Scope separation (e.g., instruction vs. metadata)
- Omission-based logic (e.g., â€œOmit if not documentedâ€)

### ðŸš« Dangerous Techniques
- â€œSummarize in your own wordsâ€ without scoping
- â€œSimplify thisâ€ in clinical/legal contexts
- Any instruction lacking an inference boundary
- Broad use of â€œrelevant,â€ â€œimportant,â€ or â€œsignificantâ€
- Catch-all prompts using â€œextract key takeawaysâ€ in regulated settings

---

## ðŸ§­ SignalFrame Principle

> *Contain hallucination by design. Not by correction.*

Prompt systems should enforce *clarity through constraint*, not after-the-fact review.

---

## References and Examples

For implementation examples, see:
- `systems/emr-triage-summary/emr-triage-summary-V2-prompt.md`
- `systems/legal-contract-extractor/legal-contract-extractor-v1-prompt.md`
